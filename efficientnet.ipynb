{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install --upgrade efficientnet-pytorch #1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys  \nsys.path.append('../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master')#这是从github上下载的\nfrom efficientnet_pytorch import EfficientNet\n#这是禁止联网时的使用方式\n# !pip install --upgrade efficientnet-pytorch","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport torch\nimport random\nimport string\nimport tifffile\nimport numpy as np \nimport pandas as pd \nimport torch.nn as nn\nfrom random import randint\nfrom torchvision import models\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.optim import lr_scheduler\nfrom efficientnet_pytorch import EfficientNet\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport warnings; warnings.filterwarnings(\"ignore\")\ngc.enable()\n\n\n#2","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":3.063995,"end_time":"2022-07-08T14:24:41.045696","exception":false,"start_time":"2022-07-08T14:24:37.981701","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)\n\n\ndebug = False\ngenerate_new = False\ntrain_df = pd.read_csv(\"../input/mayo-clinic-strip-ai/train.csv\").head(10 if debug else 329)\ntest_df = pd.read_csv(\"../input/mayo-clinic-strip-ai/test.csv\")\ndirs = [\"../input/mayo-clinic-strip-ai/train/\", \"../input/mayo-clinic-strip-ai/test/\"] #这是.tif文件所在的dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3\nmax_count = max(train_df.label.value_counts())\n#print(max_count)\nfor label in ['CE','LAA']:\n    df = train_df.loc[train_df.label == label] #train_df.label == label构成了一个布尔序列，把train_df中的对应列选出来\n    #print(df)\n    while(train_df.label.value_counts()[label] < max_count): #\n        train_df = pd.concat([train_df, df.head(max_count - train_df.label.value_counts()[label])], axis = 0)\n#these codes are for making the size of two sets equally \ntrain_df.label.value_counts()","metadata":{"papermill":{"duration":69.477711,"end_time":"2022-07-08T14:25:50.554416","exception":false,"start_time":"2022-07-08T14:24:41.076705","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset ","metadata":{}},{"cell_type":"code","source":"class ImgDataset(Dataset):\n    def __init__(self, df):\n        self.df = df \n        self.train = 'label' in df.columns    #self.train是‘是不是训练集’的意思。应该命名为is_train\n    def __len__(self): return len(self.df)    \n    def __getitem__(self, index):\n        if(1): paths = [\"./test/\", \"./train/\"] #generate_new 我现在只做output里缩小版的图片\n        image = cv2.imread(paths[self.train] + self.df.iloc[index].image_id + \".jpg\")\n\n        image = cv2.resize(image, (512, 512)).transpose(2, 0, 1)\n        label = None\n        if(self.train): label = {\"CE\" : 0, \"LAA\": 1}[self.df.iloc[index].label]\n        return image, label","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loader = DataLoader(ImgDataset(val), batch_size=batch_size, shuffle=False, num_workers=1)\ndataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\na=ImgDataset(train)\nprint(a.__getitem__(100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import zipfile\n# def zip(file):\n#   zipfile_name = os.path.basename(file) + '.zip'\n#   with zipfile.ZipFile(zipfile_name, 'w') as zfile:\n#     for foldername, subfolders, files in os.walk(file):\n#       zfile.write(foldername)\n#       for i in files:\n#         zfile.write(os.path.join(foldername, i))\n#     zfile.close()\n\n# zip('/kaggle/working') #For zipping dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \ndef train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n    tran_acc_list=[]\n    valid_acc_list=[]\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        model.cuda()       \n        flag=0\n        for phase in ['train', 'val']:\n            if phase == 'train': model.train()\n            else: model.eval()\n               \n            epoch_loss = 0.0\n            epoch_acc = 0\n            \n            dataloader = dataloaders_dict[phase]\n            for item in tqdm(dataloader, leave=False):\n                images = item[0].cuda().float()\n                classes = item[1].cuda().long()\n                optimizer.zero_grad()                \n                with torch.set_grad_enabled(phase == 'train'):\n                    output = model(images)\n                    loss = criterion(output, classes)\n                    _, preds = torch.max(output, 1)\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                    epoch_loss += loss.item() * len(output)\n                    epoch_acc += torch.sum(preds == classes.data)                    \n            data_size = len(dataloader.dataset)\n            epoch_loss = epoch_loss / data_size\n            epoch_acc = epoch_acc.double() / data_size\n            if phase=='train':\n                tran_acc_list.append(epoch_acc.item())\n            else:valid_acc_list.append(epoch_acc.item())\n            print(f'Epoch {epoch + 1}/{num_epochs} | {phase:^5} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}')    \n        if epoch_acc > best_acc:\n            traced = torch.jit.trace(model.cpu(), torch.rand(1, 3, 512, 512))\n            traced.save('model.pth')\n            best_acc = epoch_acc\n    return tran_acc_list,valid_acc_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Running the training","metadata":{}},{"cell_type":"code","source":"#model = efficientnet_pytorch.EfficientNet.from_pretrained(\"efficientnet-b4\")\nmodel = EfficientNet.from_name(\"efficientnet-b0\")\nmodel.set_swish(memory_efficient = False)\ncheckpoint = torch.load('../input/efficientnet-pytorch/efficientnet-b0-08094119.pth')\nmodel.load_state_dict(checkpoint)\nmodel.set_swish(memory_efficient = False)\n\ntrain, val = train_test_split(train_df, test_size=0.2, random_state=42, stratify = train_df.label)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_loader = DataLoader(ImgDataset(train), batch_size=1, shuffle=True, num_workers=1)\nval_loader = DataLoader(ImgDataset(val), batch_size=1, shuffle=True, num_workers=1)\ndataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\ncriterion = nn.CrossEntropyLoss()\n\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n# train_model(model, dataloaders_dict, criterion, optimizer, 1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ntrain_acc,test_acc=train_model(model, dataloaders_dict, criterion, optimizer, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 4\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import Subset\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nvalid_acc_set=[]\ntrain_acc_set=[]\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n    if fold>1:continue\n    train_set = Subset(ImgDataset(train_df), train_idx)\n    val_set = Subset(ImgDataset(train_df), val_idx)\n    \n    model = EfficientNet.from_name(\"efficientnet-b1\")\n    model.set_swish(memory_efficient = False)\n    checkpoint = torch.load('../input/efficientnet-pytorch/efficientnet-b1-dbc7070a.pth')\n    model.load_state_dict(checkpoint)\n    model.set_swish(memory_efficient = False)\n    \n    train_loader = DataLoader(train_set, batch_size=1, shuffle=True, num_workers=1)\n    val_loader = DataLoader(val_set, batch_size=1, shuffle=True, num_workers=1)\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n    criterion = nn.CrossEntropyLoss()\n\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n# train_model(model, dataloaders_dict, criterion, optimizer, 1)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n    train_acc,valid_acc=train_model(model, dataloaders_dict, criterion, optimizer, 6)\n    valid_acc_set.append(valid_acc)\n    train_acc_set.append(train_acc)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Convert tensors to numbers using item()\ntrain_acc=[]   #\n\n# Plot the training and test accuracy\nplt.plot(range(1,7), train_acc_set[1], label='training accuracy')\nplt.plot(range(1,7), valid_acc_set[0],label='validation accuracy')\nplt.ylim(0,1)\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\nprint(train_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_acc_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum=0\nfor i in valid_acc_set:\n    sum+=max(i)\navg=sum/len(valid_acc_set)\nprint(avg)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(valid_acc_set[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}